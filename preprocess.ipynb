{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "DigitsMapper = {'0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four', '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine', '10': 'ten',\n",
    "                'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7','eight': '8', 'nine': '9', 'ten': '10'}\n",
    "\n",
    "def normal_query(query, document):\n",
    "    \"\"\" normalize digits\n",
    "    \"\"\"\n",
    "    nq = []\n",
    "    for w in query:\n",
    "        if w in DigitsMapper and w not in document:\n",
    "            if DigitsMapper[w] in document:\n",
    "                w = DigitsMapper[w]\n",
    "        nq.append(w)\n",
    "    return nq\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "def token_extend(reg_rules):\n",
    "    return ' ' + reg_rules.group(0) + ' '\n",
    "\n",
    "def reform_text(text):\n",
    "    text = re.sub(u'-|¢|¥|€|£|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/', token_extend, text)\n",
    "    text = text.strip(' \\n')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter):\n",
    "    print(\"Generating {} examples...\".format(data_type))\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as fh:\n",
    "        source = json.load(fh)\n",
    "        for article in tqdm(source[\"data\"]):\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                context = para[\"context\"].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "                context_tokens = word_tokenize(reform_text(context))\n",
    "                spans = convert_idx(context, context_tokens)\n",
    "                context_tokens = [normalize_text(t) for t in context_tokens]\n",
    "                context_chars = [list(token) for token in context_tokens]\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"])\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"])\n",
    "                for qa in para[\"qas\"]:\n",
    "                    total += 1\n",
    "                    ques = qa[\"question\"].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "                    ques_tokens = word_tokenize(reform_text(ques))\n",
    "                    ques_tokens = [normalize_text(t) for t in ques_tokens]\n",
    "                    ques_tokens = normal_query(ques_tokens, context_tokens)\n",
    "                    ques_chars = [list(token) for token in ques_tokens]\n",
    "                    for token in ques_tokens:\n",
    "                        word_counter[token] += 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1\n",
    "                    y1s, y2s = [], []\n",
    "                    answer_texts = []\n",
    "                    for answer in qa[\"answers\"]:\n",
    "                        answer_text = answer[\"text\"]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        answer_texts.append(answer_text)\n",
    "                        answer_span = []\n",
    "                        for idx, span in enumerate(spans):\n",
    "                            if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                                answer_span.append(idx)\n",
    "                        y1, y2 = answer_span[0], answer_span[-1]\n",
    "                        y1s.append(y1)\n",
    "                        y2s.append(y2)\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "                    examples.append(example)\n",
    "                    eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]}\n",
    "        random.shuffle(examples)\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {}\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    if emb_file is not None:\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.split()\n",
    "                word = \"\".join(array[0:-vec_size])\n",
    "                vector = list(map(float, array[-vec_size:]))\n",
    "                if word in counter and counter[word] > limit:\n",
    "                    embedding_dict[word] = vector\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements:\n",
    "            embedding_dict[token] = [np.random.normal(\n",
    "                scale=0.1) for _ in range(vec_size)]\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 1)}\n",
    "    idx2token_dict={}\n",
    "    idx2token_dict[0]=NULL\n",
    "    idx2token_dict[len(embedding_dict)+1]=OOV\n",
    "    for k in token2idx_dict:\n",
    "        idx2token_dict[token2idx_dict[k]]=k\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = len(embedding_dict)+1\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "    embedding_dict[OOV] = np.random.random((1,vec_size))*0.2-0.1\n",
    "    idx2emb_dict = {idx: embedding_dict[token] for token, idx in token2idx_dict.items()}\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict, idx2token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 442/442 [01:11<00:00,  6.21it/s]\n",
      "  0%|                                                                                              | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:09<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "train_examples, train_eval = process_file('original_data/train-v1.1.json', \"train\", word_counter, char_counter)\n",
    "test_examples, test_eval = process_file('original_data/dev-v1.1.json', \"dev\", word_counter, char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval\n",
    "with open('dataset/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "with open('dataset/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 891/2200000 [00:00<04:06, 8906.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████▊| 2196017/2200000 [03:00<00:00, 12177.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89700 / 105846 tokens have corresponding word embedding vector\n",
      "Generating char embedding...\n",
      "1231 tokens have corresponding embedding vector\n"
     ]
    }
   ],
   "source": [
    "word_emb_mat, word2idx_dict, id2word_dict = get_embedding(\n",
    "    word_counter, \"word\", emb_file='original_data/glove.840B.300d.txt', size=int(2.2e6), vec_size=300)\n",
    "char_emb_mat, char2idx_dict, id2char_dict = get_embedding(\n",
    "        char_counter, \"char\", emb_file=None, size=None, vec_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # save id2word\n",
    "# import pandas as pd\n",
    "# df_id2word=[]\n",
    "# for k in id2word_dict:\n",
    "#     df_id2word.append([k,id2word_dict[k]])\n",
    "# df_id2word=pd.DataFrame(df_id2word)\n",
    "# df_id2word.to_csv('dataset/id2word.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_size=len(word_emb_mat)\n",
    "char_input_size=len(char_emb_mat)-1\n",
    "print(word_size)\n",
    "print(char_input_size)\n",
    "\n",
    "word_mat=np.zeros((len(word_emb_mat),len(word_emb_mat[0])))\n",
    "for i,w in enumerate(word_emb_mat):\n",
    "    word_mat[i,:]=w\n",
    "print(word_mat.shape)\n",
    "np.save('dataset/word_emb_mat.npy',word_mat)\n",
    "\n",
    "char_mat=np.zeros((len(char_emb_mat),len(char_emb_mat[0])))\n",
    "for i,w in enumerate(char_emb_mat):\n",
    "    char_mat[i,:]=w\n",
    "print('char_mat:', char_mat.shape)\n",
    "np.save('dataset/char_emb_mat.npy',char_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                              | 173/87599 [00:00<00:50, 1726.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 87599/87599 [00:43<00:00, 2005.94it/s]\n",
      "  2%|█▋                                                                             | 226/10570 [00:00<00:04, 2259.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 87341 / 87599 instances of features in total\n",
      "unanswerable: 0\n",
      "Processing dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10570/10570 [00:05<00:00, 1896.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 10471 / 10570 instances of features in total\n",
      "unanswerable: 0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, id2word_dict, \\\n",
    "                   is_test=False):\n",
    "\n",
    "    para_limit = config['test_para_limit'] if is_test else config['para_limit']\n",
    "    ques_limit = config['test_ques_limit'] if is_test else config['ques_limit']\n",
    "    ans_limit = 100 if is_test else config['ans_limit']\n",
    "    char_limit = config['char_limit']\n",
    "\n",
    "    def filter_func(example, is_test=False):\n",
    "        if len(example['y2s'])==0 or len(example['y1s'])==0:\n",
    "            print(example)\n",
    "        return len(example[\"context_tokens\"]) > para_limit or \\\n",
    "               len(example[\"ques_tokens\"]) > ques_limit or \\\n",
    "               (example[\"y2s\"][0] - example[\"y1s\"][0]) > ans_limit\n",
    "\n",
    "    print(\"Processing {} examples...\".format(data_type))\n",
    "    total = 0\n",
    "    total_ = 0\n",
    "    meta = {}\n",
    "    context_idxss=[]\n",
    "    ques_idxss=[]\n",
    "    context_char_idxss=[]\n",
    "    ques_char_idxss=[]\n",
    "    context_strings=[]\n",
    "    ques_strings=[]\n",
    "    y1s=[]\n",
    "    y2s=[]\n",
    "    y1ps=[]\n",
    "    y2ps=[]\n",
    "    qids=[]\n",
    "    unans=0\n",
    "    for example in tqdm(examples):\n",
    "        total_ += 1\n",
    "\n",
    "        if filter_func(example, is_test):\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        qids.append(int(example['id']))\n",
    "        context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "        context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "        ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "        ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "        y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "        y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "            \n",
    "\n",
    "        def _get_word(word):\n",
    "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                if each in word2idx_dict:\n",
    "                    return word2idx_dict[each]\n",
    "            return 1\n",
    "\n",
    "        def _get_char(char):\n",
    "            if char in char2idx_dict:\n",
    "                return char2idx_dict[char]\n",
    "            return 1\n",
    "        \n",
    "        cont_temp=[]\n",
    "        ques_temp=[]\n",
    "        for i, token in enumerate(example[\"context_tokens\"]):\n",
    "            context_idxs[i] = _get_word(token)\n",
    "            cont_temp.append(token)\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_tokens\"]):\n",
    "            ques_idxs[i] = _get_word(token)\n",
    "            ques_temp.append(token)\n",
    "\n",
    "        for i, token in enumerate(example[\"context_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                ques_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "        start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
    "        y1[start], y2[end] = 1.0, 1.0\n",
    "        context_idxss.append(np.expand_dims(context_idxs,axis=0))\n",
    "        ques_idxss.append(np.expand_dims(ques_idxs,axis=0))\n",
    "        context_char_idxss.append(np.expand_dims(context_char_idxs,axis=0))\n",
    "        ques_char_idxss.append(np.expand_dims(ques_char_idxs,axis=0))\n",
    "        y1s.append(np.expand_dims(y1,axis=0))\n",
    "        y2s.append(np.expand_dims(y2,axis=0))\n",
    "        context_strings.append(cont_temp)\n",
    "        ques_strings.append(ques_temp)\n",
    "        \n",
    "    context_idxss=np.concatenate(context_idxss,axis=0)\n",
    "    ques_idxss=np.concatenate(ques_idxss,axis=0)\n",
    "    context_char_idxss=np.concatenate(context_char_idxss,axis=0)\n",
    "    ques_char_idxss=np.concatenate(ques_char_idxss,axis=0)\n",
    "    y1s=np.concatenate(y1s,axis=0)\n",
    "    y2s=np.concatenate(y2s,axis=0)\n",
    "    qids=np.array(qids)\n",
    "    context_strings=np.array(context_strings)\n",
    "    ques_strings=np.array(ques_strings)\n",
    "    \n",
    "    np.save(out_file+data_type+'_contw_input.npy',context_idxss)\n",
    "    np.save(out_file+data_type+'_quesw_input.npy',ques_idxss)\n",
    "    np.save(out_file+data_type+'_contc_input.npy',context_char_idxss)\n",
    "    np.save(out_file+data_type+'_quesc_input.npy',ques_char_idxss)\n",
    "    np.save(out_file+data_type+'_y_start.npy',y1s)\n",
    "    np.save(out_file+data_type+'_y_end.npy',y2s)\n",
    "    np.save(out_file+data_type+'_qid.npy',qids)\n",
    "    with open(out_file+data_type+'_contw_strings.pkl','wb') as f:\n",
    "        pickle.dump(context_strings, f)\n",
    "    with open(out_file+data_type+'_quesw_strings.pkl','wb') as f:\n",
    "        pickle.dump(ques_strings, f)\n",
    "    \n",
    "    print(\"Built {} / {} instances of features in total\".format(total, total_))\n",
    "    print('unanswerable:',unans)\n",
    "\n",
    "config={\n",
    "    'test_para_limit':1000,\n",
    "    'test_ques_limit':50,\n",
    "    'para_limit':400,\n",
    "    'ques_limit':50,\n",
    "    'ans_limit':30,\n",
    "    'char_limit':16,\n",
    "}\n",
    "\n",
    "build_features(config, train_examples, 'train', 'dataset/', word2idx_dict, char2idx_dict, id2word_dict, is_test=False)\n",
    "build_features(config, test_examples, 'dev', 'dataset/', word2idx_dict, char2idx_dict, id2word_dict, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
